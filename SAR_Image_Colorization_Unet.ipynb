{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# UNet4.0"
      ],
      "metadata": {
        "id": "OqwHUcLfAmf_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82EgEmidAlK4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "# Paths\n",
        "dataset_root = \"/content/sentinel_sar_dataset/v_2/urban\"\n",
        "image_size = (256, 256)\n",
        "num_images = 4000  # Training with 4000 images\n",
        "\n",
        "# Load images\n",
        "def load_images(folder, num_images):\n",
        "    image_files = sorted(os.listdir(folder))[:num_images]\n",
        "    images = [img_to_array(load_img(os.path.join(folder, img), target_size=image_size)) for img in image_files]\n",
        "    return np.array(images) / 255.0  # Normalize\n",
        "\n",
        "# Load SAR (input) and Optical (target)\n",
        "X = load_images(f\"{dataset_root}/s1\", num_images)\n",
        "Y = load_images(f\"{dataset_root}/s2\", num_images)\n",
        "\n",
        "print(f\"‚úÖ Loaded {X.shape[0]} SAR images & {Y.shape[0]} Optical images.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß© Cell 1: Install and Import Dependencies"
      ],
      "metadata": {
        "id": "dtbs9tFvBBQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation-models\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "QXF-M6C7A761"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üì¶ Cell 2: Prepare Dataset (Split into Train/Val/Test)"
      ],
      "metadata": {
        "id": "pi1J1fYHBFQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming X and Y are your full datasets with 4000 images\n",
        "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.2, random_state=42)  # 80% train\n",
        "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)  # 10% val, 10% test\n"
      ],
      "metadata": {
        "id": "XqQUIJhKA73n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üåÄ Cell 3: Data Augmentation for Training Set"
      ],
      "metadata": {
        "id": "-AlAAYSmBIg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator()\n",
        "\n",
        "# Combine images and masks for flow()\n",
        "train_generator = train_datagen.flow(X_train, Y_train, batch_size=16)\n",
        "val_generator = val_datagen.flow(X_val, Y_val, batch_size=16)\n"
      ],
      "metadata": {
        "id": "iwTLw2qDA709"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Cell 4: Define Loss Functions"
      ],
      "metadata": {
        "id": "q1BdNujZBRtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vgg = VGG16(include_top=False, weights=\"imagenet\", input_shape=(256, 256, 3))\n",
        "vgg.trainable = False\n",
        "perceptual_model = Model(inputs=vgg.input, outputs=vgg.get_layer(\"block3_conv3\").output)\n",
        "\n",
        "def ssim_loss(y_true, y_pred):\n",
        "    return 1 - tf.image.ssim(y_true, y_pred, max_val=1.0)\n",
        "\n",
        "def perceptual_loss(y_true, y_pred):\n",
        "    return K.mean(K.abs(perceptual_model(y_true) - perceptual_model(y_pred)))\n",
        "\n",
        "def combined_loss(y_true, y_pred):\n",
        "    return 0.5 * ssim_loss(y_true, y_pred) + 0.5 * perceptual_loss(y_true, y_pred)\n"
      ],
      "metadata": {
        "id": "7AM95G-EA7yZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üèóÔ∏è Cell 5: Define U-Net Model (With Dropout + L2)"
      ],
      "metadata": {
        "id": "AqmQSpnfBUvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unet_model(input_shape=(256, 256, 3)):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(1e-4))(inputs)\n",
        "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(1e-4))(c1)\n",
        "    p1 = MaxPooling2D((2, 2))(c1)\n",
        "    p1 = Dropout(0.3)(p1)\n",
        "\n",
        "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(1e-4))(p1)\n",
        "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(1e-4))(c2)\n",
        "    p2 = MaxPooling2D((2, 2))(c2)\n",
        "    p2 = Dropout(0.3)(p2)\n",
        "\n",
        "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(1e-4))(p2)\n",
        "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(1e-4))(c3)\n",
        "    p3 = MaxPooling2D((2, 2))(c3)\n",
        "    p3 = Dropout(0.3)(p3)\n",
        "\n",
        "    # Bottleneck\n",
        "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(1e-4))(p3)\n",
        "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(1e-4))(c4)\n",
        "\n",
        "    # Decoder\n",
        "    u5 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c4)\n",
        "    u5 = concatenate([u5, c3])\n",
        "    u5 = Dropout(0.3)(u5)\n",
        "    c5 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(1e-4))(u5)\n",
        "    c5 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(1e-4))(c5)\n",
        "\n",
        "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "    u6 = concatenate([u6, c2])\n",
        "    u6 = Dropout(0.3)(u6)\n",
        "    c6 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(1e-4))(u6)\n",
        "    c6 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(1e-4))(c6)\n",
        "\n",
        "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "    u7 = concatenate([u7, c1])\n",
        "    u7 = Dropout(0.3)(u7)\n",
        "    c7 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(1e-4))(u7)\n",
        "    c7 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(1e-4))(c7)\n",
        "\n",
        "    outputs = Conv2D(3, (1, 1), activation='sigmoid')(c7)\n",
        "\n",
        "    return Model(inputs, outputs)\n"
      ],
      "metadata": {
        "id": "REdEFeXxA7vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß™ Cell 6: Compile & Train with EarlyStopping + Checkpoint"
      ],
      "metadata": {
        "id": "Mnvn9KOyBbzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = unet_model()\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss=combined_loss, metrics=['mae'])\n",
        "\n",
        "early_stop = EarlyStopping(patience=5, restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint(\"best_unet_model.h5\", save_best_only=True)\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=50,\n",
        "    callbacks=[early_stop, checkpoint]\n",
        ")\n"
      ],
      "metadata": {
        "id": "KB4Lfl5iA7tD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìä Cell 7: Final Evaluation on Test Set"
      ],
      "metadata": {
        "id": "JZARdssTBhkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = model.evaluate(X_test, Y_test)\n",
        "print(\"Test Loss:\", test_loss)\n"
      ],
      "metadata": {
        "id": "8SEoAujuBhPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üåü Cell 8: BONUS ‚Äì Use Pretrained U-Net (ResNet34 encoder)"
      ],
      "metadata": {
        "id": "p-5U0P29Bmt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from segmentation_models import Unet\n",
        "from segmentation_models.backbones import get_preprocessing\n",
        "\n",
        "preprocess_input = get_preprocessing('resnet34')\n",
        "\n",
        "X_train_prep = preprocess_input(X_train)\n",
        "X_val_prep = preprocess_input(X_val)\n",
        "X_test_prep = preprocess_input(X_test)\n",
        "\n",
        "model_resnet = Unet(\n",
        "    backbone_name='resnet34',\n",
        "    encoder_weights='imagenet',\n",
        "    classes=3,\n",
        "    activation='sigmoid'\n",
        ")\n",
        "\n",
        "model_resnet.compile(optimizer=Adam(learning_rate=0.001), loss=combined_loss, metrics=['mae'])\n",
        "\n",
        "history_resnet = model_resnet.fit(\n",
        "    X_train_prep, Y_train,\n",
        "    validation_data=(X_val_prep, Y_val),\n",
        "    epochs=50,\n",
        "    batch_size=16,\n",
        "    callbacks=[early_stop, checkpoint]\n",
        ")\n",
        "test_loss = model_resnet.evaluate(X_test, Y_test)\n",
        "print(\"Test Loss:\", test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "collapsed": true,
        "id": "Xu3fjU-PBdAe",
        "outputId": "bdc2c064-8806-403e-cd7f-e919e5c9479f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'segmentation_models'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-804e21c795ae>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msegmentation_models\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msegmentation_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbones\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpreprocess_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'resnet34'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'segmentation_models'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2dCknKHDBc89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o_FemBPzBc6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8T4D5juiBc4N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}